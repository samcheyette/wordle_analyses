---
title: "Wordle analyses"
output: html_document
---




```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,warning=FALSE,message=FALSE,fig.width=5, fig.height=4,fig.align = "center",cache=TRUE)
```


```{r, echo=FALSE, include=FALSE, warning=FALSE, message=FALSE}
##libraries, globals

library(ggplot2)
library(reshape)
library(grid)
library(dplyr)
library(gridExtra)
library(lme4)
library(reghelper)
library(RColorBrewer)
#library(e1071)   
library(robustbase)
#library(tidylog)
library(hash)



paper_theme <- theme_light() + theme( axis.title.x = element_text(size=18),
  axis.text.x=element_text(colour="#292929", 
                           size = 14), 
  axis.title.y = element_text(size = 18, vjust = 1),
  axis.text.y  = element_text(size = 14, colour="#292929"),
  strip.text=element_text(size=16,color="black"),
strip.background = element_rect(colour = "grey50", fill = "white"),
panel.background = element_rect(fill = "white", colour = "grey50"),
  axis.ticks.x=element_blank(),axis.ticks.y=element_blank(),
  axis.line.x = element_line(colour = "black"), 
  axis.line.y = element_line(colour = "black"),
  legend.title=element_text(size=18),
  legend.text=element_text(size=15),
  panel.grid.major = element_blank(), panel.grid.minor = element_blank())


```


```{r}
df <- read.csv("data/data.csv")
df$guess <- as.character(df$guess)
df$true_word <- as.character(df$true_word)
df$words_remaining_post <- df$words_remaining_post + 1*(df$words_remaining_post == 0)
df$words_remaining_pre <- df$words_remaining_pre + 1*(df$words_remaining_pre == 0)

df$bonus_regime <- gsub("TIME","Time",df$bonus_regime)
df$bonus_regime <- gsub("GUESS","Guess",df$bonus_regime)

df <- df %>%
      mutate(corr = 1*(guess==true_word)) %>%
      filter(time_elapsed_curr >= 0) %>%
      group_by(pid) %>%
      mutate(mean_time_spent=mean(time_elapsed_curr)) %>%
      group_by(pid,trial_id) %>%
      mutate(ever_corr = max(guess==true_word)) %>%
      mutate(total_guesses = max(guess_number)) %>%
      mutate(total_trial_time = max(time_elapsed_trial)) %>%
      filter(total_trial_time < 60 * 10) %>%
      filter(max(time_elapsed_curr) < 60 * 5) %>%
      mutate(prop_words_remaining = words_remaining_post/words_remaining_pre) %>%
      group_by(true_word) %>%
      mutate(mean_time_curr=mean(time_elapsed_curr))


df_sub <- df %>%
      group_by(pid, trial_id) %>%
      top_n(n=1,wt=guess_number) %>%
      group_by(true_word) %>%
      mutate(mean_guess_number = mean(guess_number)) %>%
      mutate(mean_corr = mean(corr))
      

max_trial <- max(df$number_of_guesses)

bonus_regime <- c()
guess_number <- c()
corr <- c()
words_remaining <- c()

for (i in 1:nrow(df_sub)) {
  corr_on <- df_sub[i,]$guess_number
  br <- as.character(df_sub[i,]$bonus_regime)
  uid <- as.character(df_sub[i,]$uniqueid)
  tid <- as.character(df_sub[i,]$trial_id)
  if (df_sub[i,]$ever_corr == FALSE) {
    corr_on <- corr_on + 1
  }
  for (j in 1:max_trial) {
    guess_number[length(guess_number)+1] <- j
    bonus_regime[length(bonus_regime)+1] <- br
    corr[length(corr)+1] <- 1*(j >= corr_on)

    if (j <= corr_on) {
      df_sub2 <- subset(df, (df$uniqueid == uid) & (df$trial_id == tid) & (df$guess_number == j))
      wr <- df_sub2$words_remaining_post
    } else {
      wr <- 1
    }
    words_remaining[length(words_remaining)+1] <- wr

  }
}
df_plt <- data.frame(bonus_regime, guess_number, corr, words_remaining)




```



```{r}

df_model <- read.csv("data/data_model6.csv")
df_model$condition <- gsub("TIME","Time",df_model$condition)
df_model$condition <- gsub("GUESS","Guess",df_model$condition)

f_get_prev_exp <- function(uid) {
  uid <- uid[1]
  subs <- subset(df_sub, as.character(df_sub$uniqueid) == uid)
  return(subs$prev_exp[1])
}

df_model <- df_model %>%
        mutate(max_v_random_gain=(max_ent_reduction+1e-5)/(random_ent_reduction+1e-5)) %>%
        group_by(uniqueid,trial) %>%
        mutate(time_elapsed_trial=cumsum(time_elapsed)) %>%
         filter(max(time_elapsed_trial) < 60 * 10) %>%
        filter(max(time_elapsed) < 60 * 5)

df_model <- df_model %>%
            group_by(uniqueid) %>%
            mutate(prev_exp = f_get_prev_exp(uniqueid))

df_model_sub <- df_model %>%
          group_by(uniqueid,trial) %>%
          top_n(n=1,wt=guess_number)





```
<br>

The question I'm interested in here is, when making a decision, 1) how do people decide how much data to gather; and 2) how much thought to put into the data they gather. One interesting idea is that under some circumstances, there will be a trade-off between gathering more data and thinking about the data. For instance, there are cases where you can figure something out by brute force without too much effort (e.g., if the space of possibilities is small). In other cases, brute force strategies will take too long, but by thinking, you can effectively narrow the space of possibilities. My hypothesis is that people choose information-gathering strategies that are efficient relative to the time costs of exploration versus thinking. A related hypothesis is that people will think more when there is more to be gained by thinking.

To test this, I ran a small pilot where participants played Wordle, where participants were in one of two incentive conditions. In one condition they were given a bonus if they found the correct word quickly. In the other condition, they were given a bonus if they guessed the correct word in few guesses. 

The first question we can ask is: did the manipulation work? There are a couple ways we can evaluate this. First, we can check whether people in the guess condition require fewer guesses in the guess-bonus condition.

<br>

```{r, fig.width=8,fig.height=3}

p.1 <- ggplot(data=df_sub, aes(x=bonus_regime, y=guess_number)) +
      #geom_jitter(width=0.1,height=0.1,alpha=0.5) +
      stat_summary(fun.data="mean_se",geom="errorbar",width=0.1) +
      stat_summary(fun="mean",geom="point",size=3) +
      stat_summary(fun="mean",geom="point",size=2,color="white") +
      paper_theme + ylab("Number of guesses") +
      scale_x_discrete(labels=c("Guess bonus","Time bonus")) +
      paper_theme + theme(axis.text.x=element_text(size=16,color="black"),
                          axis.title.x=element_blank())

p.2 <- ggplot(data=df, aes(x=bonus_regime, y=time_elapsed_curr)) +
      #geom_jitter(width=0.1,height=0.1,alpha=0.5) +
      stat_summary(fun.data="mean_se",geom="errorbar",width=0.1) +
      stat_summary(fun="mean",geom="point",size=3) +
      stat_summary(fun="mean",geom="point",size=2,color="white") +
      paper_theme + ylab("Time per guess (s)") +
      scale_x_discrete(labels=c("Guess bonus","Time bonus")) +
      paper_theme + theme(axis.text.x=element_text(size=16,color="black"),
                          axis.title.x=element_blank())

grid.arrange(p.1, p.2, ncol=2)

```
<br><br>

We can look at the probability of guessing the correct word (left) and the total time elapsed (right) in each condition after *n* guesses:
<br>

```{r, fig.width=8,fig.height=3}


p.1 <- ggplot(data=df_plt, aes(x=guess_number, y=corr, color=bonus_regime))+
      stat_summary(fun="mean",geom="line") +
      stat_summary(fun="mean",geom="point") +
      stat_summary(fun.data="mean_se",geom="errorbar",width=0.1) +
      guides(color=guide_legend(title="Bonus")) +
      scale_color_manual(values=c("blue","orange"), labels=c("Guess", "Time")) +
      paper_theme +theme(legend.position=c(0.78,0.3)) +
      xlab("Guess number") + ylab("P(correct)")

p.2 <- ggplot(data=df, aes(x=guess_number, y=time_elapsed_trial,color=bonus_regime)) +
      stat_summary(fun="mean",geom="line") +
      stat_summary(fun="mean",geom="point") +
      stat_summary(fun.data="mean_se",geom="errorbar",width=0.1) +
      guides(color="none") +
      scale_color_manual(values=c("blue","orange"), labels=c("Guess", "Time")) +
      paper_theme +theme(legend.position=c(0.18,0.78)) +
      xlab("Guess number") + ylab("Time elapsed (s)") 




grid.arrange(p.1, p.2, ncol=2)

```
<br><br>

We can now ask a slightly tricker question: were people allocating time to thinking *efficiently*? One way of answering this question is to look at whether people spent more time thinking when there was more information to gain. Specifically, we will compare the quality of participants' guesses as a function of the relative information gain from a very good guess versus a random guess. The graph on the left below shows the proportion of possible guesses that a participant's guess was better than (i.e., was more informative than) as a function of the relative information gain from the best possible guess versus a random guess. The graph on the right shows the same thing broken down by condition. We see a very strong effect of relative gain from a good sample on the quality of participant's guesses:

<br>


```{r, fig.width=8,fig.height=3.25}

p.1 <- ggplot(data=subset(df_model, df_model$guess_number > 1), aes(x=max_v_random_gain, y=quantile_ent_reduction)) +
      stat_summary_bin(binwidth=0.25) +
      stat_smooth(method="lm", formula=y~poly(x,1)) +
      paper_theme  + theme(legend.title=element_blank(), legend.position=c(0.78,0.88)) +
     coord_trans(y="log") +
      xlab("Max vs. random gain") + ylab("Quantile info gain") 



p.2 <- ggplot(data=subset(df_model, df_model$guess_number > 1), aes(x=max_v_random_gain, y=quantile_ent_reduction, color=condition)) +
      stat_summary_bin(binwidth=0.4) +
      stat_smooth(method="lm", formula=y~poly(x,1)) +
      paper_theme  + theme(legend.title=element_blank(), legend.position=c(0.18,0.85)) +
      guides(color=guide_legend(title="Bonus")) +
      scale_color_manual(values=c("blue","orange"), labels=c("Guess", "Time")) +
     coord_trans(y="log") +
      xlab("Max vs. random gain") + ylab("Quantile info gain") 

grid.arrange(p.1, p.2, ncol=2)

```
<br><br>


We can also check whether this same pattern holds at each guess number. That is, whether this effect is just due to people having a policy for how much to think after 1, 2, 3, etc... guesses, or whether it's actually more complex than than that. What we see is that this effect holds within each guess number:

<br>

```{r}


ggplot(data=subset(df_model, df_model$guess_number < 10), aes(x=max_v_random_gain, y=quantile_ent_reduction)) +
      stat_summary_bin(binwidth=0.5) +
      stat_smooth(method="lm", formula=y~poly(x,1)) +
      paper_theme  + theme(legend.title=element_blank(), legend.position=c(0.78,0.88)) +
      xlab("Max vs. random gain") + ylab("Quantile info gain") + 
      facet_wrap(~guess_number)

```
<br><br>

Finally, we can estimate how many guesses participants were effectively sampling, based on how good their guess was. The plot below shows the effective number of samples taken as a function of the max vs. random information gain.

<br>

```{r}


ggplot(data=subset(df_model, df_model$guess_number > 1), aes(x=max_v_random_gain, y=eff_samples)) +
      stat_summary_bin(binwidth=0.25) +
      stat_smooth(method="lm", formula=y~poly(x,1)) +
      paper_theme  + theme(legend.title=element_blank(), legend.position=c(0.78,0.88)) +
     #coord_trans(y="log") +
      xlab("Max vs. random gain") + ylab("Effective samples") 



```


